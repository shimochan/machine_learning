# 機械学習
- [機械学習](#機械学習)
  - [はじめに](#はじめに)
  - [データの取得](#データの取得)
  - [データ前処理](#データ前処理)
    - [外れ値除去](#外れ値除去)
    - [テキストの修正](#テキストの修正)
    - [データの誤りを修正](#データの誤りを修正)
  - [特徴量エンジニアリング](#特徴量エンジニアリング)
    - [欠損値の扱い](#欠損値の扱い)
    - [数値変数の変換](#数値変数の変換)
    - [カテゴリ変数の変換](#カテゴリ変数の変換)
    - [日付の変換](#日付の変換)
    - [変数の組み合わせ](#変数の組み合わせ)
    - [他のテーブルの結合](#他のテーブルの結合)
    - [集約して統計量を取る](#集約して統計量を取る)
    - [時系列データの扱い](#時系列データの扱い)
    - [自然言語処理](#自然言語処理)
  - [選定モデル](#選定モデル)
  - [学習、評価、予測](#学習評価予測)

## はじめに
参考文献：[Kaggleで勝つデータ分析の技術](https://www.amazon.co.jp/Kaggle%E3%81%A7%E5%8B%9D%E3%81%A4%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E6%8A%80%E8%A1%93-%E9%96%80%E8%84%87-%E5%A4%A7%E8%BC%94/dp/4297108437/ref=sr_1_2_sspa?adgrpid=109872001568&gclid=CjwKCAjw6dmSBhBkEiwA_W-EoOd1a4VqeM4ox27d_m3rL-iUCm4pEVnrTmqKUUdI2NpxgQONDEUnOhoC-VMQAvD_BwE&hvadid=592006298558&hvdev=c&hvlocphy=1009129&hvnetw=g&hvqmt=e&hvrand=15866788662565985356&hvtargid=kwd-818170082585&hydadcr=27492_14541079&jp-ad-ap=0&keywords=kaggle%E3%81%A7%E5%8B%9D%E3%81%A4%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E6%8A%80%E8%A1%93&qid=1649866337&sr=8-2-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExT0NTMllDMldDVTBRJmVuY3J5cHRlZElkPUEwMzUzNDk0QUtSTUtKNUk5UTgwJmVuY3J5cHRlZEFkSWQ9QTFJRkQ0UFo2VVRDOUYmd2lkZ2V0TmFtZT1zcF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl)  
今回は機械学習の中でも`教師あり学習`に限定して、以下の処理をテキストベースでまとめる。回帰・分類タスクで共通した手法を取り上げるので、回帰・分類で変わる評価指標は細かく触れない。  
また、本来であればEDAを行ったうえで機械学習を行うのだが、今回は割愛する。

## データの取得
前提条件
- データが取得できる状態まで整形されていること

基本は`pandas`ライブラリを使用
- [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
- [read_excel()](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)


## データ前処理
前処理の種類としては以下が挙げられるが、どれを実施するするかはデータと相談。

### 外れ値除去
  - 手作業による検出・削除
  - 四分位範囲（IQR）から、範囲外を削除
    - IQR : Q1 - Q3
    - 下側境界：Q1 - IQR * 1.5
    - 上側境界：Q3 + IQR * 1.5
      - 参考：[【pandas】四分位数を求めて外れ値を検出する](https://qiita.com/koga1020/items/9710cb3f7f65a00808e4)
  - Σスコアから、範囲外を削除
    - 絶対値3を基準にする
### テキストの修正
  - 単語単位で修正
  - 英語9割日本語1割のテキストの場合、一部日本語を英語に翻訳することもある。
    - 参考：[Pythonでgoogletransを使って翻訳](https://qiita.com/optimisuke/items/e44e66c2646e763d1f7b)
### データの誤りを修正
  - 例：年齢150歳(現実ではありえない) -> 100歳に修正

## 特徴量エンジニアリング
機械学習の肝になる部分。ここでいかに目的変数に関係ある特徴量を作れるかで予測精度が大きく変わる。  
以下で挙げる内容は大体Pythonライブラリで簡単に実装可能な内容である。  
その他重要そうな特徴量を作成したい場合は、ベースライン作成後の特徴量重要度を見るなり、EDAで関係性を見つけるなり、ドメイン知識から特徴量作成等の作業が必要。  
参考  
[序盤に試すテーブルデータの特徴量エンジニアリング](https://zenn.dev/colum2131/articles/fffac4654e7c7c)  
[kaggleで勝つデータ分析の技術](https://gihyo.jp/book/2019/978-4-297-10843-4)

### 欠損値の扱い
  - そのまま使用、GBDT(xgboost, LightGBM)なら可能
  - 各列の平均、中央値等で補完
  - 行ごと削除
  - 他の変数から欠損値を予測する
  - 欠損値から新たな特徴量を作成
    - 欠損かどうかを表す二値変数
    - レコードごとに欠損している変数の数をカウント
    - 複数の欠損の組み合わせ

### 数値変数の変換
GBDT(xgboose, LightGBM)だと数値自体の変換のみではほとんど影響がないので、以下の方法の多くは適用しても意味がありません。  
ですが、[集約して統計量を取る](#集約して統計量を取る)場合があり、その前処理変換として行うことは意味があります。

  - 標準化(standardization)
    - 平均0、標準偏差を1にする変換。
    - 参考：[StandardScalerを試してみる](https://qiita.com/nobodytolove123/items/9cec78b65a9d5f3a39fd)
  - Min-Maxスケーリング
    - 変数の取る範囲を特定の区間に押し込める変換
    - 変換後の平均が0にならない、外れ値の影響を受けやすい等のデメリットがあるため、標準化の方がよく使われる印象
    - 参考：[[Python MinMaxScaler] 0 ~ 1に正規化](https://qiita.com/Qiitaman/items/c94420e8b86aae5f28a9)
  - 対数、絶対値の対数を取る
    - 金額やカウントを表す変数では、一方向に袖が伸びた分布になりがちなので、対数変換を行うことで正規分布に近づけることがある。
  - clipping
    - 上限や下限を設定し、それを外れた値は上限や下限の値に変換
  - binning
    - 区間ごとにグループ分けして、あえてカテゴリ変数として扱う
    - 基本はbinningの後に[カテゴリ変数の変換](#カテゴリ変数の変換)を行う
  - 順位付け
    - 数値の大きさや間隔の情報をあえて捨てる方法

### カテゴリ変数の変換
多くの機械学習モデルでは、そのまま分析に用いることができません。なので、モデルごとに適した形に変換が必要になります。  
また、データ上は数値であっても値の大きさや順序に意味がない場合は、カテゴリ変数として扱うべきなので注意が必要。

  - one-hot encoding
    - カテゴリ変数の各水準に対して、その水準かどうかを表す二値変数をそれぞれ作成する
    - 欠点：水準数が多い場合にはほとんどの値が0になり、情報が少ない特徴量が大量に生成される
  - label encoding
    - 各水準を単純に整数に置き換える方法
    - 決定木をベースにした手法以外は、label encodingはあまり適切ではない
    - 逆にGBDTにおいては、label encodingは基本的な方法
  - feature hashing
    - ハッシュ関数を用いてカテゴリ変数を指定した次元のベクトルに圧縮
    - one-hot encodingでは特徴量の数が多すぎる場合に利用できる
  - frequency encoding
    - 各水準の出現回数・出現頻度でカテゴリ変数を変換
  - target encoding
    - 目的変数を用いてカテゴリ変数を数値に変換
    - 目的変数をリークさせてしまう可能性があるため、実装方法に工夫が必要
    - 参考：[Target Encodingで精度向上させた例(Leave One Out)](https://qiita.com/FukuharaYohei/items/88cc562ab7800cbc01cb)
    - 参考：[カテゴリ変数のエンコーディング](https://qiita.com/ground0state/items/f516b97c7a8641e474c4)


### 日付の変換
  - 年月日に分ける
    - 年：有効かどうかは、データの分割方法や性質にかなり依存する
    - 月：1年間の季節性をとらえることができる
    - 日：月の中で周期的な傾向がある場合は期待できる
  - 曜日
    - 基本はlabel encoding
  - 休日
    - 休日フラグ
  - 祝日
    - 祝日フラグ or 休日フラグ
    - 翌日や翌々日が休日か
    - 前日や前々日が休日か
  - 特別な日
    - 正月・クリスマス・ゴールデンウィーク・ブラックフライデー等
    - 特別な日フラグや、前後の日を特別フラグ
  - 時間差
    - 株：前回の配当から何日経っているか

### 変数の組み合わせ
EDAしつつ、ドメイン知識・データの背景を考慮して、組み合わせないと意味をなさない変数を生成してしまうので注意。  

  - 数値×カテゴリ
    - カテゴリ変数の水準ごとに、数値の平均や分散等の統計を取る
    - カテゴリ変数の水準ごとに、数値の標準化をする
  - 数値×数値
    - GBDTでは加減をしてもあまり効果が出ないが、乗除は割と効果あり
  - カテゴリ×カテゴリ
    - 新たなカテゴリを生成して、target encodingを適用
  - 行の統計量をとる
    - レコードごとに複数の変数を対象として統計量をとる
    - 意味を考えた上で一部の変数に絞る方が有効だと考えられる
    - 欠損・ゼロ・負の値カウント
    - 平均、分散、最大、最小等の統計量

### 他のテーブルの結合
  - 1対1で対応している場合
    - 例：商品IDが1対1で対応している場合は、商品IDをキーで結合
  - 1対多
    - データの統計量を取るなどして集約し、1対1のデータに変換する

### 集約して統計量を取る
  - 単純な統計
    - カウント
    - ユニーク数のカウント
    - 存在するか
    - 合計・平均・割合
    - 最大・最小・標準偏差・中央値・分位点・尖度・歪度
  - 時間的な統計量
    - 直近や最初のレコードの情報
    - 間隔・頻度
  - 条件を絞る
    - 特定の種類のログに絞る
    - 購入したした、商品をお気に入りに入れた等の特定イベントに絞る
    - 集計時間・期間を絞る
      - 朝・昼・夕方・夜
      - 直近1週間、1ヶ月

### 時系列データの扱い
以下の観点から、どのように扱えばよいか考えてみよう。
  1. 時間情報を持つ変数があるかどうか
     1. 時間情報を上手く使って特徴量作成
  2. 学習データ・テストデータが時系列で分かれているか
     1. 時間に沿って分割したバリデーションを行うとともに、将来の情報を不適切に使わないように注意
  3. ユーザや店舗といった系列ごとに時系列の目的変数があり
     1. 過去の目的変数が将来の予測に重要な情報であるため、[ラグ特徴量](https://qiita.com/birdwatcher/items/c94e42d94451c2eaa3fc)を作ることになる


  - 予測する時点より過去の情報のみを使う
    - 将来のデータを不適切に使うと、目的変数のリークを起こす可能性がある
  - 過去の情報のみを使う制約を緩める
    - データ不足で十分な学習できない場合
    - 時系列的な性質が弱い変数
  - ワイドフォーマットとロングフォーマット
    - ワイド->ロング：pandasのstack
    - ロング->ワイド：pandasのpivot

### 自然言語処理
テーブルデータでも、name等でテキストデータが存在することもある。  
そのテキストからさらに特徴量を生成する手法を紹介する。
参考：[テーブルデータ向けの自然言語特徴抽出術](https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1)

  - bag-of-words
    - 文章などのテキストを単語に分割し、単語の出現数をカウント
    - scikit-learnのCountVectorizerがある
  - tf-idf
    - bag-of-words後に、一般的な単語の重要度を下げ、特定の単語の重要度を上げるよう重みづけした手法
    - scikit-learnのTfidfTransformerがある
  - 次元圧縮
    - CountVectorizerやTfidfTransformerだと、数万次元生成されるため、一般的には学習が難しい
    - SVD/NMF/LDAを使用して、低次元に圧縮できる


## 選定モデル
まずはLightGBMで良い。
進めていくうちに、ニューラルネットを検討したり、アンサンブルやスタッキングを考える場合はその他モデルも作成する。

  - GBDT(LightGBM, xgboost)
    - 使いやすさ・高速・精度の高さが売り
    - 欠損値をそのまま扱いことができる
    - 変数間の相互作用が反映される
    - 不要な特徴量を追加しても精度が落ちにくい
  - ニューラルネット
    - テーブルデータに対しては、中間層が2~4程度の全結合層からなる多層パーセプロトンがよく使われている
    - 欠損値を扱うことはできない
    - 基本的には特徴量を標準化などでスケーリングする必要がある
    - ハイパーパラメータの調整が難しい
    - 他クラス分類に比較的強い
    - GPUで高速化
  - 線形モデル
    - 単体だと精度が出ないため、アンサンブルの1つやスタッキングの最終層に適用する等の使い道がほとんど
    - 回帰タスク：線形回帰モデル
    - 分類タスク：ロジスティック回帰モデル
    - 特徴量は基本的には標準化が必要
    - 特徴量を作るときに丁寧な処理が必要
  - k近傍法
    - レコード間の距離を特徴量の値の差を用いて定義し、その距離が最も近いk個のレコードの目的変数から回帰、分類
    - 特徴量は基本的には標準化が必要
  - ランダムフォレスト
    - 決定木の集合により予測を行うが、GBDTとは違い並列に決定木を作成


## 学習、評価、予測
  - 基本的な分析コンペでの学習と検証の流れ
    1. モデルとハイパーパラメータを指定
    2. 学習データでモデルの学習・検証データでモデルを評価
    3. 学習したモデルでテストデータに対して予測し、予測値の提出

  - 学習と検証データの分割方法
    - hold-out法
      - 一部のデータを検証データとして取り分けておく
    - クロスバリデーション
      - 学習データをいくつかに分割
      - そのうちの1つを検証データ、残りを学習データとして学習・評価
      - 分割した回数だけ、検証データを変えて学習・評価
      - それらのスコアの平均でモデルの良し悪しを評価
      - K-fold
        - 最もよく使う基本的なクロスバリデーション
      - Stratified k-fold
        - 各foldに含まれるクラスの割合を等しくする
        - foldのクラスの割合が偏ってしまうような場合に有効
      - Group k-fold
        - 同じグループ(顧客や被験者など特定の人物を表すものなど)が同じfoldになるようにデータを分割する
  - 学習と評価が終わったら
    - 特徴量の追加・変更
    - 特徴量の重要性見てみる(LightGBMなら可視化が簡単)
    - ハイパーパラメータを変更する
    - モデルの種類を変更する

  - モデルに関連する用語とポイント
    - 過学習(オーバーフィッティング)
      - 学習データではスコアが良いが、それ以外のデータではスコアが悪くなること
    - 正則化
      - 学習時にモデルが複雑な時に罰則を科すこと
      - モデルの多くは正則化の強さを指定するハイパーパラメータを調整することで過学習を抑えられる
    - アーリーストッピング
      - 学習時に検証データのスコアをモニタリングし、一定の間スコアが向上しない場合、学習を打ち切る機能
