※現在作成中

# 機械学習
- [機械学習](#機械学習)
  - [はじめに](#はじめに)
  - [データの取得](#データの取得)
  - [データ前処理](#データ前処理)
    - [外れ値除去](#外れ値除去)
    - [テキストの修正](#テキストの修正)
    - [データの誤りを修正](#データの誤りを修正)
  - [特徴量エンジニアリング](#特徴量エンジニアリング)
    - [欠損値の扱い](#欠損値の扱い)
    - [数値変数の変換](#数値変数の変換)
    - [カテゴリ変数の変換](#カテゴリ変数の変換)
    - [日付・時刻を粟原素変数の変換](#日付時刻を粟原素変数の変換)
    - [変数の組み合わせ](#変数の組み合わせ)
    - [他のテーブルの結合](#他のテーブルの結合)
    - [集約して統計量を取る](#集約して統計量を取る)
    - [時系列データの扱い](#時系列データの扱い)
    - [次元削除・教師なし学習による特徴量](#次元削除教師なし学習による特徴量)
    - [自然言語処理](#自然言語処理)

## はじめに
今回は機械学習の中でも`教師あり学習`に限定して、以下の処理をテキストベースでまとめる。回帰・分類タスクで共通した手法を取り上げるので、回帰・分類で変わる評価指標やモデルの選定等は細かく触れない。  
また、本来であればEDAを行ったうえで機械学習を行うのだが、今回は割愛する。

1. データの取得
2. データ前処理
3. 特徴量エンジニアリング
4. 学習アルゴリズム選定
5. 訓練用データと検証用データに分ける
6. 学習・スコア検証
7. テストデータで予測


## データの取得
前提条件
- データが取得できる状態まで整形されていること

基本は`pandas`ライブラリを使用
- [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
- [read_excel()](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)


## データ前処理
前処理の種類としては以下が挙げられるが、どれを実施するするかはデータと相談。

### 外れ値除去
  - 手作業による検出・削除
  - 四分位範囲（IQR）から、範囲外を削除
    - IQR : Q1 - Q3
    - 下側境界：Q1 - IQR * 1.5
    - 上側境界：Q3 + IQR * 1.5
      - 参考：[【pandas】四分位数を求めて外れ値を検出する](https://qiita.com/koga1020/items/9710cb3f7f65a00808e4)
  - zスコアから、範囲外を削除
    - 絶対値3を基準にする
### テキストの修正
  - 単語単位で修正
  - 英語9割日本語1割のテキストの場合、一部日本語を英語に翻訳することもある。
    - 参考：[Pythonでgoogletransを使って翻訳](https://qiita.com/optimisuke/items/e44e66c2646e763d1f7b)
### データの誤りを修正
  - 年齢150歳 -> 100歳に修正

## 特徴量エンジニアリング
機械学習の肝になる部分。ここでいかに目的変数に関係ある特徴量を作れるかで予測精度が大きく変わる。  
以下で挙げる内容は大体Pythonライブラリで簡単に実装可能な内容である。  
その他重要そうな特徴量を作成したい場合は、ベースライン作成後の特徴量重要度を見るなり、EDAで関係性を見つけるなり、ドメイン知識から特徴量作成等の作業が必要。

### 欠損値の扱い
  - そのまま使用、GBDT(xgboost, LightGBM)なら可能
  - 各列の平均、中央値等で補完
  - 行ごと削除
  - 他の変数から欠損値を予測する
  - 欠損値から新たな特徴量を作成
    - 欠損かどうかを表す二値変数
    - レコードごとに欠損している変数の数をカウント
    - 複数の欠損の組み合わせ

### 数値変数の変換
GBDT(xgboose, LightGBM)だと数値自体の変換のみではほとんど影響がないので、以下の方法の多くは適用しても意味がありません。  
ですが、[集約して統計量を取る](#集約して統計量を取る)場合があり、その前処理変換として行うことは意味があります。

  - 標準化(standardization)
    - 平均0、標準偏差を1にする変換。
    - 参考：[StandardScalerを試してみる](https://qiita.com/nobodytolove123/items/9cec78b65a9d5f3a39fd)
  - Min-Maxスケーリング
    - 変数の取る範囲を特定の区間に押し込める変換
    - 変換後の平均が0にならない、外れ値の影響を受けやすい等のデメリットがあるため、標準化の方がよく使われる印象
    - 参考：[[Python MinMaxScaler] 0 ~ 1に正規化](https://qiita.com/Qiitaman/items/c94420e8b86aae5f28a9)
  - 対数、絶対値の対数を取る
    - 金額やカウントを表す変数では、一方向に袖が伸びた分布になりがちなので、対数変換を行うことで正規分布に近づけることがある。
  - Box-Cox変換
    - パラメータλを指定しないと、正規分布らしい分布に変換
  - Yeo-Johnson変換
    - 負の値に対応したBox-Cox変換
  - clipping
    - 上限や下限を設定し、それを外れた値は上限や下限の値に変換
  - binning
    - 区間ごとにグループ分けして、あえてカテゴリ変数として扱う
    - 基本はbinningの後に[カテゴリ変数の変換](#カテゴリ変数の変換)を行う
  - 順位付け
    - 数値の大きさや感覚の情報をあえて捨てる方法

### カテゴリ変数の変換


### 日付・時刻を粟原素変数の変換


### 変数の組み合わせ


### 他のテーブルの結合


### 集約して統計量を取る


### 時系列データの扱い


### 次元削除・教師なし学習による特徴量


### 自然言語処理














